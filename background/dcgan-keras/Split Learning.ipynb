{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Loaded model...\n",
      "Test loss: 0.03083601578749367\n",
      "Test accuracy: 0.9899\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./model_weights.h5\"\n",
    "if os.path.exists(model_path):\n",
    "    model = load_model(model_path)\n",
    "    print(\"Loaded model...\")\n",
    "else:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    model.save(model_path)\n",
    "    print(\"Saved model...\")\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seperated = {}\n",
    "y_train_seperated = {}\n",
    "for target_label in range(10):\n",
    "    x_train_seperated[target_label] = []\n",
    "    y_train_seperated[target_label] = []\n",
    "    for i, label in enumerate(y_train):\n",
    "        if list(label).index(1) == target_label:\n",
    "            x_train_seperated[target_label].append(x_train[i])\n",
    "            y_train_seperated[target_label].append(y_train[i])\n",
    "            \n",
    "x_test_seperated = {}\n",
    "y_test_seperated = {}\n",
    "for target_label in range(10):\n",
    "    x_test_seperated[target_label] = []\n",
    "    y_test_seperated[target_label] = []\n",
    "    for i, label in enumerate(y_test):\n",
    "        if list(label).index(1) == target_label:\n",
    "            x_test_seperated[target_label].append(x_test[i])\n",
    "            y_test_seperated[target_label].append(y_test[i])\n",
    "            \n",
    "x_train_seperated = [np.array(x_train_seperated[i]) for i in range(10)]\n",
    "y_train_seperated = [np.array(y_train_seperated[i]) for i in range(10)]\n",
    "x_test_seperated = [np.array(x_test_seperated[i]) for i in range(10)]\n",
    "y_test_seperated = [np.array(y_test_seperated[i]) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE from https://stackoverflow.com/questions/46772685/how-to-accumulate-gradients-in-tensorflow\n",
    "\n",
    "## Optimizer definition - nothing different from any classical example\n",
    "opt = tf.train.AdamOptimizer()\n",
    "\n",
    "## Retrieve all trainable variables you defined in your graph\n",
    "tvs = tf.trainable_variables()\n",
    "## Creation of a list of variables with the same shape as the trainable ones\n",
    "# initialized with 0s\n",
    "accum_vars = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]\n",
    "zero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
    "\n",
    "## Calls the compute_gradients function of the optimizer to obtain... the list of gradients\n",
    "gvs = opt.compute_gradients(rmse, tvs)\n",
    "\n",
    "## Adds to each element from the list you initialized earlier with zeros its gradient (works because accum_vars and gvs are in the same order)\n",
    "accum_ops = [accum_vars[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]\n",
    "\n",
    "## Define the training step (part with variable value update)\n",
    "train_step = opt.apply_gradients([(accum_vars[i], gv[1]) for i, gv in enumerate(gvs)])\n",
    "\n",
    "\n",
    "\n",
    "## The while loop for training\n",
    "while ...:\n",
    "    # Run the zero_ops to initialize it\n",
    "    sess.run(zero_ops)\n",
    "    # Accumulate the gradients 'n_minibatches' times in accum_vars using accum_ops\n",
    "    for i in xrange(n_minibatches):\n",
    "        sess.run(accum_ops, feed_dict=dict(X: Xs[i], y: ys[i]))\n",
    "    # Run the train_step ops to update the weights based on your accumulated gradients\n",
    "    sess.run(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on client 0 using class 0\n",
      "Train on 5923 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5923/5923 [==============================] - 10s 2ms/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0787 - val_acc: 0.9785\n",
      "Training on client 1 using class 1\n",
      "Train on 6742 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "6742/6742 [==============================] - 13s 2ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.1955 - val_acc: 0.9495\n",
      "Training on client 2 using class 2\n",
      "Train on 5958 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5958/5958 [==============================] - 12s 2ms/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.1860 - val_acc: 0.9480\n",
      "Training on client 3 using class 3\n",
      "Train on 6131 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "6131/6131 [==============================] - 13s 2ms/step - loss: 0.0247 - acc: 0.9936 - val_loss: 0.3797 - val_acc: 0.8882\n",
      "Training on client 4 using class 4\n",
      "Train on 5842 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5842/5842 [==============================] - 12s 2ms/step - loss: 0.0166 - acc: 0.9957 - val_loss: 0.2254 - val_acc: 0.9198\n",
      "Training on client 5 using class 5\n",
      "Train on 5421 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5421/5421 [==============================] - 11s 2ms/step - loss: 0.0214 - acc: 0.9939 - val_loss: 0.2290 - val_acc: 0.9283\n",
      "Training on client 6 using class 6\n",
      "Train on 5918 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5918/5918 [==============================] - 12s 2ms/step - loss: 0.0104 - acc: 0.9966 - val_loss: 0.1574 - val_acc: 0.9506\n",
      "Training on client 7 using class 7\n",
      "Train on 6265 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "6265/6265 [==============================] - 13s 2ms/step - loss: 0.0082 - acc: 0.9971 - val_loss: 0.4221 - val_acc: 0.8782\n",
      "Training on client 8 using class 8\n",
      "Train on 5851 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5851/5851 [==============================] - 12s 2ms/step - loss: 0.0260 - acc: 0.9925 - val_loss: 0.4264 - val_acc: 0.8780\n",
      "Training on client 9 using class 9\n",
      "Train on 5949 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5949/5949 [==============================] - 12s 2ms/step - loss: 0.0502 - acc: 0.9854 - val_loss: 0.2904 - val_acc: 0.9055\n"
     ]
    }
   ],
   "source": [
    "model_split = load_model(model_path)\n",
    "\n",
    "# train model on every class, iterately (one client per class)\n",
    "for label in range(10):\n",
    "    print('Training on client {} using class {}'.format(label, label))\n",
    "    model_split.fit(x_train_seperated[label], y_train_seperated[label],\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "split_model_path = \"./split_model_weights.h5\"\n",
    "if os.path.exists(model_path):\n",
    "    model = load_model(model_path)\n",
    "    print(\"Loaded model...\")\n",
    "else:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    model.save(model_path)\n",
    "    print(\"Saved model...\")\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (Label Poisoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0239 - accuracy: 0.9924 - val_loss: 0.0235 - val_accuracy: 0.9932\n",
      "Epoch 2/12\n",
      "23168/60000 [==========>...................] - ETA: 2s - loss: 0.0215 - accuracy: 0.9921"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-62a4a344082e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0madv_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./adv_model_weights.h5\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = \"./model_weights.h5\"\n",
    "adv_model = load_model(model_path)\n",
    "print(\"Loaded model...\")\n",
    "\n",
    "adv_model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "adv_model_path = \"./adv_model_weights.h5\"\n",
    "adv_model.save(adv_model_path)\n",
    "print(\"Saved model...\")\n",
    "\n",
    "score = adv_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
